{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Optimizer Demo\n",
    "\n",
    "## Tool description\n",
    "In short, the Graph-Optimizer tool performs the following functions:\n",
    "- Predicts the execution time (in milliseconds) and energy consumption (in Joules) for a given BGO or DAG of BGOs on a specific hardware configuration.\n",
    "- Returns the model in symbolical form with graph properties as symbols or predicts execution times if the graph properties are specified.\n",
    "- This is done via an API where issuing a POST request to `<api_url>/models` with the BGO DAG and hardware configuration returns an annotated DAG with calibrated symbolical models. Calling `<api_url>/evaluate` with the BGO DAG, hardware configuration, and graph properties returns an annotated DAG with predicted execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a BGO?\n",
    "A BGO, or Basic Graph Operation, is an atomic graph operation, that can serve as a building block for constructing larger graph _workloads_.\n",
    "A single BGO can have multiple implementations, possibly targeting different hardware platforms (e.g., CPU or GPU).\n",
    "A workload is a Directed Acyclic Graph (DAG) of BGOs, where the nodes are BGOs and the edges represent data dependencies between them.\n",
    "An example of such a DAG is shown below:\n",
    "\n",
    "<img style=\"margin-bottom: -245px\" src=\"dag.svg\">\n",
    "\n",
    "In this DAG, we start with the Betweenness Centrality (BC) BGO, followed by the Breadth First Search (BFS) and Find Max BGO's. Finally, we have the Find Path BGO to conclude. This example is a workload that outputs the path from the root node to the node with the highest betweenness centrality, which is the most popular node in the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance modeling\n",
    "\n",
    "Each implementation of a BGO has a symbolic model that describes its execution time and energy consumption as functions of graph properties and hardware characteristics. Specifically, these hardware characteristics refer to the execution times of atomic operations in hardware, or operations considered to be atomic, such as reading a value from memory, writing a value to memory, performing an integer addition, and so on. These characteristics are obtained through _microbenchmarks_ run on the hardware where the BGO will be executed. The values obtained from the microbenchmarks are used to calibrate the symbolic models, which then provide the execution time and energy consumption of the BGO based solely on the graph properties. This approach allows for predicting the execution time and energy consumption of a BGO on a specific hardware configuration without needing to run the BGO on the hardware for any particular input graph.\n",
    "\n",
    "Such a calibrated model might look like this:\n",
    "\n",
    "$T_{BGO} = 561n \\times 924m + 91n^2$,\n",
    "\n",
    "where $n$ is the number of nodes in a graph, $m$ is the number of edges in the graph, and $T_{BGO}$ is the execution time of the BGO in milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "The following are the requirements to run this Graph-Optimizer demo.\n",
    "You can either install them manually using the commands provided, or run the following code cell, which will install them automatically:\n",
    "- **Flask**: The prediction server makes use of _Flask_ to serve the API. Install it using `pip install Flask`.\n",
    "- **ipywidgets**: This notebook uses _ipywidgets_ for interactive experiments. Install it using `pip install ipywidgets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "# Flask is required to run the prediction API\n",
    "!pip install Flask\n",
    "# Widgets are used later in this file for interacting with the data\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the Graph-Optimizer tool\n",
    "### Step 1: Specify input DAG of BGO's\n",
    "\n",
    "The first step in using Graph Optimizer is to specify the input DAG of BGO's. From the above list, select the BGO's you want to use and specify the input DAG.\n",
    "This DAG should include one or multiple BGOs and their dependencies. The BGO name should match the name of the BGO folder in the `models` directory.\n",
    "There are currently four BGO's available. These are:\n",
    "- bc (Betweenness Centrality)\n",
    "- bfs (Breadth First Search)\n",
    "- find_max (Find Max)\n",
    "- find_path (Find Path)\n",
    "\n",
    "The dependencies should be specified as a list of BGO id's that the current BGO depends on. For instance, consider the following example with multiple BGOs and dependencies, representing the DAG shown in the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dag = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"name\": \"bc\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"find_max\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"bfs\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"find_path\",\n",
    "        \"dependencies\": [1,2]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Specify hardware configuration\n",
    "The hardware configuration aims to describe all available hardware components in a system or data center. The hardware information is used for the calibration of the performance models.\n",
    "\n",
    "To specify the hardware configuration, you need to provide the configuration in JSON format. This configuration should list all unique available hosts in the data center, including details about CPUs and, if applicable, GPUs. Running microbenchmarks is part of this process and is done automatically with a script. An example configuration is provided below.\n",
    "\n",
    "##### Alter the values in the hardware configuration to represent your own system. You can skip the microbenchmarks for now, as these will be done automatically later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = {\n",
    "    \"hosts\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"host1\",\n",
    "            \"cpus\": {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"intel xeon\",\n",
    "                \"clock_speed\": 2.10,\n",
    "                \"cores\": 16,\n",
    "                \"threads\": 32,\n",
    "                \"wattage\": 250,\n",
    "                \"amount\": 2,\n",
    "                \"benchmarks\": {\n",
    "                    \"T_int_add\": 2.4, \"T_float_gt\": 0.8,\n",
    "                    \"T_q_pop\": 11.2, \"T_q_push\": 16.1, \"T_q_front\": 14.5,\n",
    "                    \"T_L1_read\": 1.26, \"T_L2_read\": 4.24, \"T_L3_read\": 20.9, \"T_DRAM_read\": 62.5,\n",
    "                    \"L1_linesize\": 64, \"L2_linesize\": 64, \"L3_linesize\": 64,\n",
    "                    \"T_heap_insert_max\": 52.7, \"T_heap_extract_min\": 123.3, \"T_heap_decrease_key\": 12.7,\n",
    "                    \"T_push_back\": 12\n",
    "                }\n",
    "            },\n",
    "            \"gpus\": [\n",
    "                {\n",
    "                    \"id\": 1,\n",
    "                    \"name\": \"RTX 3080\",\n",
    "                    \"clock_speed\": 2.10,\n",
    "                    \"cores\": 400,\n",
    "                    \"warps\": 32,\n",
    "                    \"wattage\": 100\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automated microbenchmarks\n",
    "\n",
    "Running the following python cell will run the microbenchmarks on your machine (this should take a couple of seconds, probably no longer than a minute), and insert them into the hardware configuration.\n",
    "\n",
    "The resulting values are the measured values for each operation in nanoseconds.\n",
    "\n",
    "**Note**: for running the microbenchmarks, g++ is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running microbenchmarks...\n",
      "{\n",
      "    \"T_int_add\": 0.3037103,\n",
      "    \"T_float_gt\": 0.08339923,\n",
      "    \"T_q_push\": 17.36117,\n",
      "    \"T_q_front\": 13.52021,\n",
      "    \"T_q_pop\": 11.4853,\n",
      "    \"T_heap_insert_max\": 39.21334,\n",
      "    \"T_heap_extract_min\": 111.0356,\n",
      "    \"T_heap_decrease_key\": 9.931942,\n",
      "    \"T_push_back\": 16.48665,\n",
      "    \"L1_linesize\": 64,\n",
      "    \"L2_linesize\": 64,\n",
      "    \"L3_linesize\": 64,\n",
      "    \"T_L1_read\": 1.5057122112056045,\n",
      "    \"T_L2_read\": 7.364671471125293,\n",
      "    \"T_L3_read\": 35.641843373372105,\n",
      "    \"T_DRAM_read\": 97.6322315890179\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from benchmarking.microbenchmarks import all_benchmarks\n",
    "import json\n",
    "\n",
    "# Run microbenchmarks on this machine\n",
    "local_benchmarks = all_benchmarks()\n",
    "\n",
    "# Pretty print the benchmarks\n",
    "print(json.dumps(local_benchmarks, indent=4))\n",
    "\n",
    "# Assign obtained values to the hardware description\n",
    "hardware[\"hosts\"][0][\"cpus\"][\"benchmarks\"] = local_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the prediction server\n",
    "\n",
    "The next step is running the prediction server, and getting the performance models for the input DAG for your specific hardware configurations.\n",
    "This step can be divided into two subtasks. First we start the prediction server, and then we submit a POST request to the server to get the performance models.\n",
    "\n",
    "1. Start the server using flask, by running the following command from the root directory:\n",
    "    ```bash\n",
    "    flask --app api/api.py run\n",
    "    ```\n",
    "    _(If you are using windows, use `python -m flask --app api/api.py run`)_\n",
    "\n",
    "    This will start the server on `localhost:5000`. Use the following command to start the server on a different port:\n",
    "    ```bash\n",
    "    flask --app api/api.py run --port <port_number>\n",
    "    ```\n",
    "    You can either do this step, or run the python cell below, which will start the server for you.\n",
    "    - **Note**: If you run the server via the cell below, make sure to wait for the server to start before proceeding to the next cells.\n",
    "    - If you are finished with the experiments, you can stop the server by running the last cell in this document, which will terminate the server.\n",
    "2. Run the prediction by submitting a POST request to the api\n",
    "    - For obtaining the calibrated symbolical models, issue a post request to `localhost:<port_number>/models`, with the following post data:\n",
    "        - \"input_dag\": The input BGO DAG in JSON format, as a string.\n",
    "        - \"hardware\": The hardware configuration in JSON format, as a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/BartD/Documents/PhD/shared_VM/graph-optimizer/models/prediction.py\n",
      " * Serving Flask app 'api/api.py'\n",
      " * Debug mode: off\n",
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [12/Jul/2024 11:29:44] \"POST //models HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [12/Jul/2024 11:29:52] \"POST //models HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# Start the server\n",
    "import multiprocessing\n",
    "\n",
    "port = 5000\n",
    "\n",
    "def run_server():\n",
    "    !flask --app api/api.py run --port {port}\n",
    "\n",
    "server = multiprocessing.Process(target=run_server)\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "When submitting the post request to the API, the response will be the input DAG, but annotated with the calibrated symbolical models. The models will be in the form of a string representing a mathematical formula, with the graph properties as parameters.\n",
    "\n",
    "For demonstration purposes, a dropdown and slider are provided below, which allow you to change the microbenchmarking parameters and see the impact they have on the performance models.\n",
    "\n",
    "#### Submit a request to the prediction server by executing the cells below, and observe how the models change when altering the microbenchmarking parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = f'http://localhost:{port}/'\n",
    "\n",
    "def models_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag)\n",
    "    }\n",
    "    models_response = requests.post(url + '/models', data=form_data)\n",
    "    return models_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Change the value of certain microbenchmarks, and see how they impact the performance models"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f472d17ef7524ae4bd602624404be712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='Variable', options=('T_int_add', 'T_float_gt', 'T_q_push', 'T_q_fr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2253670ec9b14fd996e0f669a79e823a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.3037103, description='Value', step=0.01), Output()), _dom_classes=('…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dropdown and slider functionality\n",
    "from ipywidgets import interact, dlink, Dropdown, FloatSlider\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "microbenchmarks = hardware['hosts'][0]['cpus']['benchmarks']\n",
    "microbenchmark_name = list(microbenchmarks.keys())[0]\n",
    "dropdown = Dropdown(options=microbenchmarks.keys(), description='Variable')\n",
    "slider = FloatSlider(min=0, max=100, step=0.01, description='Value', value=microbenchmarks[microbenchmark_name])\n",
    "\n",
    "def set_microbenchmark_name(variable):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = variable\n",
    "\n",
    "def update_slider_value(x):\n",
    "    slider.value = microbenchmarks[x]\n",
    "    return slider.value\n",
    "\n",
    "def update_microbenchmark_value(value):\n",
    "    print(value)\n",
    "    hardware['hosts'][0]['cpus']['benchmarks'][microbenchmark_name] = value\n",
    "    response = models_request()\n",
    "    # print(response)\n",
    "\n",
    "dlink((dropdown, 'value'), (slider, 'value'), update_slider_value)\n",
    "display(Markdown('### Change the value of certain microbenchmarks, and see how they impact the performance models'))\n",
    "interact(set_microbenchmark_name, variable=dropdown)\n",
    "interact(update_microbenchmark_value, value=slider);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Specify graph characteristics _(optional)_\n",
    "\n",
    "The final step is to specify the graph characteristics of a specific graph for which you want to predict the execution time. This can be done by submitting a POST request to the API with the input DAG, hardware configuration, and graph properties. The API will return the input DAG annotated with the predicted execution times.\n",
    "\n",
    "The graph properties are expressed in a simple JSON format of which an example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_props = {\n",
    "    \"n\": 15763,\n",
    "    \"m\": 171206,\n",
    "    \"average_degree\": 21,\n",
    "    \"directed\": False,\n",
    "    \"weighted\": False,\n",
    "    \"diameter\": 7,\n",
    "    \"clustering_coefficient\": 0.0132526,\n",
    "    \"triangle_count\": 591156,\n",
    "    \"s\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag),\n",
    "        'graph_props': json.dumps(graph_props)\n",
    "    }\n",
    "    evaluate_response = requests.post(url + '/evaluate', data=form_data)\n",
    "    clear_output(wait=True)\n",
    "    print(evaluate_response.text)\n",
    "\n",
    "def set_num_nodes(value):\n",
    "    graph_props[\"n\"] = value\n",
    "    evaluate_request()\n",
    "\n",
    "display(Markdown('### Change the number of nodes in the graph, and see how it impacts the performance and energy predictions'))\n",
    "interact(set_num_nodes, value=FloatSlider(min=100, max=100000, step=1, description='#nodes', value=graph_props[\"n\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "server.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware configuration comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
