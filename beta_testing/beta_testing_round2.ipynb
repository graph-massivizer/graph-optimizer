{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-Optimizer beta testing round 2\n",
    "\n",
    "## Tool description\n",
    "In short, the Graph-Optimizer tool performs the following functions:\n",
    "- Predicts the execution time (in milliseconds) and energy consumption (in Joules) for a given BGO or DAG of BGOs on a specific hardware configuration.\n",
    "- Returns the model in symbolical form with graph properties as symbols or predicts execution times if the graph properties are specified.\n",
    "- This is done via an API where issuing a POST request to `<api_url>/models` with the BGO DAG and hardware configuration returns an annotated DAG with calibrated symbolical models. Calling `<api_url>/evaluate` with the BGO DAG, hardware configuration, and graph properties returns an annotated DAG with predicted execution times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What is a BGO?\n",
    "A BGO, or Basic Graph Operation, is an atomic graph operation, that can serve as a building block for constructing larger graph _workloads_.\n",
    "A single BGO can have multiple implementations, possibly targeting different hardware platforms (e.g., CPU or GPU).\n",
    "A workload is a Directed Acyclic Graph (DAG) of BGOs, where the nodes are BGOs and the edges represent data dependencies between them.\n",
    "An example of such a DAG is shown below:\n",
    "\n",
    "<img style=\"margin-bottom: -245px\" src=\"dag.svg\">\n",
    "\n",
    "In this workflow, we start with the Betweenness Centrality (BC) BGO, followed by the Breadth First Search (BFS) and Find Max BGO's. Finally, we have the Find Path BGO to conclude. This example is a workload that outputs the path from the root node to the node with the highest betweenness centrality, which is the most popular node in the graph.\n",
    "\n",
    "#### BGO implementations\n",
    "\n",
    "Each BGO can have multiple implementations, on different hardware devices. These different implementations can have varying performance based on the system. The task of the optimizer is to identify which BGO implementation is optimal for a given system. In this tutorial, we have various CPU and GPU implementations for the BGO's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance modeling\n",
    "\n",
    "#### Analytical modeling\n",
    "An implementation of a BGO can have a symbolic model that describes its execution time and energy consumption as functions of graph properties and hardware characteristics. Specifically, these hardware characteristics refer to the execution times of atomic operations in hardware, or operations considered to be atomic, such as reading a value from memory, writing a value to memory, performing an integer addition, and so on. These characteristics are obtained through _microbenchmarks_ run on the hardware where the BGO will be executed. The values obtained from the microbenchmarks are used to calibrate the symbolic models, which then provide the execution time and energy consumption of the BGO based solely on the graph properties. This approach allows for predicting the execution time and energy consumption of a BGO on a specific hardware configuration without needing to run the BGO on the hardware for any particular input graph.\n",
    "\n",
    "Such a calibrated model might look like this:\n",
    "\n",
    "$T_{BGO} = 561n \\times 924m + 91n^2$,\n",
    "\n",
    "where $n$ is the number of nodes in a graph, $m$ is the number of edges in the graph, and $T_{BGO}$ is the execution time of the BGO in milliseconds.\n",
    "\n",
    "#### Sampling based prediction\n",
    "Analytical models can provide valuable insights into performance, but are difficult and time consuming to make accurate. Therefore, a second prediction approach is implemented. By sampling the graph to a fraction of its full size, and running a BGO on this sampled version, we can observe the execution time of the BGO on the sampled graph, and multiply this by some scalar, which depends on the size of the graph and sampling rate, to get a prediction for the execution time of the BGO on the full graph. A big drawback of sampling based prediction is the prediction overhead. Whereas analytical modeling gives instant predictions, sampling based prediction does give a significant overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Includes for the entire notebook\n",
    "import multiprocessing\n",
    "import requests\n",
    "import subprocess\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "os.chdir(\"/workspace\")\n",
    "\n",
    "import networkx as nx\n",
    "import scipy as sp\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from ipywidgets import interact, dlink, Dropdown, FloatSlider, ToggleButtons\n",
    "from ipyfilechooser import FileChooser\n",
    "from IPython.display import Markdown, display, clear_output\n",
    "\n",
    "from beta_testing.plot_predictions import plot_annotated_dag, analytical_prediction_validation, sampling_prediction_validation\n",
    "from benchmarks.microbenchmarks import all_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the Graph-Optimizer tool\n",
    "### Step 1: Specify input DAG of BGO's\n",
    "\n",
    "The first step in using Graph Optimizer is to specify the input DAG of BGO's. From the above list, select the BGO's you want to use and specify the input DAG.\n",
    "This DAG should include one or multiple BGOs and their dependencies. The BGO name should match the name of the BGO folder in the `models` directory.\n",
    "There are currently 7 BGO's available. These are:\n",
    "- bc (Betweenness Centrality)\n",
    "- pr (PageRank)\n",
    "- bfs (Breadth First Search)\n",
    "- find_max (Find Max)\n",
    "- find_path (Find Path)\n",
    "- cc (Connected Components)\n",
    "- tc (Triangle Counting)\n",
    "\n",
    "The dependencies should be specified as a list of BGO id's that the current BGO depends on. For instance, consider the following example with multiple BGOs and dependencies, representing the DAG shown in the introduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dag = [\n",
    "    {\n",
    "        \"id\": 0,\n",
    "        \"name\": \"pr\",\n",
    "        \"dependencies\": []\n",
    "    },\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"find_max\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"bfs\",\n",
    "        \"dependencies\": [0]\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"find_path\",\n",
    "        \"dependencies\": [1,2]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using this configuration for this tutorial, but feel free to play around with the different BGO's after completing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Step 2: Specify hardware configuration\n",
    "The hardware configuration aims to describe all available hardware components in a system or data center. The hardware information is used for the calibration of the performance models.\n",
    "\n",
    "To specify a custom hardware configuration, you need to provide the configuration in JSON format. This configuration should list all unique available hosts in the data center, including details about CPUs and, if applicable, GPUs. Running microbenchmarks is part of this process and is done automatically with a script. An example configuration is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hardware = {\n",
    "    \"hosts\": [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"name\": \"H01\",\n",
    "            \"cpus\": {\n",
    "                \"id\": 1,\n",
    "                \"name\": \"intel xeon\",\n",
    "                \"clock_speed\": 2.10,\n",
    "                \"cores\": 16,\n",
    "                \"threads\": 32,\n",
    "                \"wattage\": 35,\n",
    "                \"amount\": 2,\n",
    "                \"benchmarks\": {\n",
    "                    \"T_int_add\": 1.739769,\n",
    "                    \"T_float_add\": 0.4340147,\n",
    "                    \"T_q_push\": 7.381785,\n",
    "                    \"T_heap_insert_max\": 37.04162,\n",
    "                    \"T_push_back\": 5.958142,\n",
    "                    \"T_DRAM_read\": 66.34084759860137\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtaining hardware information\n",
    "\n",
    "The following code cell will fetch the hardware information for the system that this notebook is running on, and fill it in in the hardware configuration JSON object.\n",
    "\n",
    "***Note:*** depending on your system, the following cell might give an error. If this is the case, please fill in the hardware information manually to the best of your knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lscpu = {k.strip(): v.strip() for item in subprocess.check_output(['lscpu']).decode('utf-8').split('\\n') for k, _, v in [item.partition(':')]}\n",
    "hardware['hosts'][0]['cpus']['name'] = lscpu['Model name']\n",
    "hardware['hosts'][0]['cpus']['cores'] = int(lscpu['Core(s) per socket'])\n",
    "hardware['hosts'][0]['cpus']['threads'] = int(lscpu['Thread(s) per core']) * hardware['hosts'][0]['cpus']['cores']\n",
    "hardware['hosts'][0]['cpus']['amount'] = int(lscpu['Socket(s)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Automated microbenchmarks\n",
    "\n",
    "Running the following python cell will run the microbenchmarks on your machine (this should take a couple of seconds, probably no longer than a minute), and insert them into the hardware configuration.\n",
    "\n",
    "The resulting values are the measured values for each operation in nanoseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run microbenchmarks on this machine\n",
    "local_benchmarks = all_benchmarks()\n",
    "\n",
    "# Pretty print the benchmarks\n",
    "print(json.dumps(local_benchmarks, indent=4))\n",
    "\n",
    "# Assign obtained values to the hardware description\n",
    "hardware[\"hosts\"][0][\"cpus\"][\"benchmarks\"] = local_benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run the prediction server\n",
    "\n",
    "The next step is running the prediction server, and getting the performance models for the input DAG for your specific hardware configurations.\n",
    "This step can be divided into two subtasks. First we start the prediction server, and then we submit a POST request to the server to get the performance models.\n",
    "\n",
    "1. Start the server using flask, by running the following command from the root directory:\n",
    "    ```bash\n",
    "    flask --app api/api.py run\n",
    "    ```\n",
    "    _(If you are using windows, use `python -m flask --app api/api.py run`)_\n",
    "\n",
    "    This will start the server on `localhost:5000`. Use the following command to start the server on a different port:\n",
    "    ```bash\n",
    "    flask --app api/api.py run --port <port_number>\n",
    "    ```\n",
    "    You can either do this step, or run the python cell below, which will start the server for you.\n",
    "    - **Note**: If you run the server via the cell below, make sure to wait for the server to start before proceeding to the next cells.\n",
    "    - If you are finished with the experiments, you can stop the server by running the last cell in this document, which will terminate the server.\n",
    "2. Run the prediction by submitting a POST request to the api\n",
    "    - For obtaining the calibrated symbolical models, issue a post request to `localhost:<port_number>/models`, with the following post data:\n",
    "        - \"input_dag\": The input BGO DAG in JSON format, as a string.\n",
    "        - \"hardware\": The hardware configuration in JSON format, as a string.\n",
    "\n",
    "***Note:*** If at any point later in this tutorial the jupyter kernel is interrupted, the server will stop. It needs to be manually activated by running this cell again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the server\n",
    "port = 5000\n",
    "url = f'http://localhost:{port}/'\n",
    "\n",
    "def run_server():\n",
    "    !flask --app api/api.py run --port {port}\n",
    "\n",
    "server = multiprocessing.Process(target=run_server)\n",
    "server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Interpreting the results\n",
    "\n",
    "When submitting the post request to the API, the response will be the input DAG, but annotated with the calibrated symbolical models. The models will be in the form of a string representing a mathematical formula, with the graph properties as parameters.\n",
    "\n",
    "For demonstration purposes, a dropdown and slider are provided below, which allow you to change the microbenchmarking parameters and see the impact they have on the performance models.\n",
    "\n",
    "#### Submit a request to the prediction server by executing the cells below, and observe how the models change when altering the microbenchmarking parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def models_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag)\n",
    "    }\n",
    "    models_response = requests.post(url + '/models', data=form_data)\n",
    "    return models_response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropdown and slider functionality\n",
    "clear_output()\n",
    "\n",
    "microbenchmarks = hardware['hosts'][0]['cpus']['benchmarks']\n",
    "microbenchmark_name = list(microbenchmarks.keys())[0]\n",
    "dropdown = Dropdown(options=microbenchmarks.keys(), description='Variable')\n",
    "slider = FloatSlider(min=0, max=200, step=0.01, description='Value', value=microbenchmarks[microbenchmark_name])\n",
    "response = None\n",
    "\n",
    "def set_microbenchmark_name(variable):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = variable\n",
    "\n",
    "\n",
    "def update_slider_value(x):\n",
    "    global microbenchmark_name\n",
    "    microbenchmark_name = x\n",
    "    slider.value = microbenchmarks[x]\n",
    "    return slider.value\n",
    "\n",
    "\n",
    "def update_microbenchmark_value(value):\n",
    "    global response\n",
    "    hardware['hosts'][0]['cpus']['benchmarks'][microbenchmark_name] = value\n",
    "    print(models_request())\n",
    "\n",
    "\n",
    "dlink((dropdown, 'value'), (slider, 'value'), update_slider_value)\n",
    "display(Markdown('### Change the value of certain microbenchmarks, and see how they impact the performance models'))\n",
    "interact(set_microbenchmark_name, variable=dropdown)\n",
    "interact(update_microbenchmark_value, value=slider);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Specify graph characteristics\n",
    "\n",
    "The final step is to specify the graph characteristics of a specific graph for which you want to predict the execution time. This can be done by submitting a POST request to the API with the input DAG, hardware configuration, and graph properties. The API will return the input DAG annotated with the predicted execution times.\n",
    "\n",
    "The graph properties are expressed in a simple JSON format of which an example is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_props = {\n",
    "    \"n\": 15763,\n",
    "    \"m\": 171206,\n",
    "    \"average_degree\": 21,\n",
    "    \"directed\": False,\n",
    "    \"weighted\": False,\n",
    "    \"clustering_coefficient\": 0.0132526,\n",
    "    \"triangle_count\": 591156,\n",
    "    \"s\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful when analyzing theoretical performance of an algorithm on non-existing graphs. However, when predicting performance for a specific graph, a graph file can be given, of which the properties are automatically extracted. Try getting analytical performance prediction results for a graph by picking one with the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser('data/beta_testing')\n",
    "fc.filter_pattern = '*.mtx'\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = fc.selected\n",
    "g = nx.from_scipy_sparse_array(sp.io.mmread(graph_name))\n",
    "\n",
    "# Extract graph properties from file\n",
    "n = len(g.nodes())\n",
    "m = len(g.edges())\n",
    "extracted_properties = {\n",
    "    \"n\": n,\n",
    "    \"m\": m,\n",
    "    \"directed\": g.is_directed(),\n",
    "    \"weighted\": nx.is_weighted(g),\n",
    "    \"diameter\": nx.diameter(g),\n",
    "    \"clustering_coefficient\": nx.average_clustering(g),\n",
    "    \"triangle_count\": sum(nx.triangles(g).values()) // 3\n",
    "}\n",
    "\n",
    "print(\"Extracted graph properties:\", extracted_properties)\n",
    "graph_props = extracted_properties\n",
    "\n",
    "def evaluate_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag),\n",
    "        'graph_props': json.dumps(graph_props)\n",
    "    }\n",
    "    evaluate_response = requests.post(url + '/evaluate', data=form_data)\n",
    "    return evaluate_response.text\n",
    "\n",
    "def plot_result(target):\n",
    "    data = json.loads(evaluate_request())\n",
    "    plot_annotated_dag(data, \"Analytical performance predictions\", target)\n",
    "\n",
    "display(Markdown('### Change the graph in the cell above, and see how it impacts the performance and energy predictions'))\n",
    "interact(plot_result, target=ToggleButtons(options=['runtime', 'energy'], description='Prediction target'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, a lot of the implementations do not have an analytical model yet, as indicated by the red labels. To still get performance predictions for these implementations, we resort to sampling based performance prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling based performance prediction\n",
    "This sampling based performance prediction works best on larger graphs. Therefore, we will continue with the higgs_social_network.mtx graph. Since calculating the graph properties can take a while, the pre-calculated properties are already listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = \"data/beta_testing/large/higgs_social_network.mtx\"\n",
    "graph_props = {\n",
    "    \"n\": 456290,\n",
    "    \"m\": 12508244,\n",
    "    \"average_degree\": 27.41292599,\n",
    "    \"directed\": False,\n",
    "    \"weighted\": False,\n",
    "    \"diameter\": 9,\n",
    "    \"clustering_coefficient\": 0.1887,\n",
    "    \"triangle_count\": 83023401,\n",
    "    \"s\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling is done by randomly selecting a certain fraction of the nodes while reading the graph. Execute the graph sampling by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = 0.1\n",
    "sampled_graph_name = 'sampled.mtx'\n",
    "!./sampling/main {graph_name} {sampling_rate} {sampled_graph_name} && echo \"Succesfully sampled the graph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell contains some functions for running benchmarks on the BGO implementations\n",
    "def run_benchmark(bgo, graph_file):\n",
    "    result = [item for item in subprocess.check_output(['python3', './autobench/run_bench.py', bgo, '--data', f'G={graph_file}']).decode('utf-8').rstrip().split('\\n') if item != ''][-2:]\n",
    "    values = dict(zip(*map(lambda x: x.split(','), result)))\n",
    "\n",
    "    return values['runtime_ns'], values['energy_joules']\n",
    "\n",
    "def benchmark_workflow(workflow, graph_file):\n",
    "    devices = [\"CPU\", \"GPU\"]\n",
    "\n",
    "    # Benchmark all bgos given in the dag, by going into all subdirectories of ./bgo/dag/CPU and ./bgo/dag/GPU, and checking if ./bench exists. If it exists, call run_benchmark with the bgo path and given graph file\n",
    "    results = []\n",
    "    for bgo in workflow:\n",
    "        bgo_name = bgo[\"name\"]\n",
    "        bgo_result = {\"name\": bgo_name, \"performances\": []}\n",
    "        performance_entry = {\"host\": \"H01\", \"runtime\": {\"CPU\": {}, \"GPU\": {}}, \"energy\": {\"CPU\": {}, \"GPU\": {}}}\n",
    "\n",
    "        base_dir = f\"./bgo/{bgo_name}\"\n",
    "        for device in devices:\n",
    "            device_dir = os.path.join(base_dir, device)\n",
    "            if not os.path.isdir(device_dir):\n",
    "                continue\n",
    "\n",
    "            # Iterate over implementations inside each device directory\n",
    "            for impl in os.listdir(device_dir):\n",
    "                impl_path = os.path.join(device_dir, impl)\n",
    "                bench_exe = os.path.join(impl_path, \"bench\")\n",
    "\n",
    "                if os.path.isfile(bench_exe) and os.access(bench_exe, os.X_OK):\n",
    "                    clear_output(wait=True)\n",
    "                    display(f\"Benchmarking {bgo_name} [{device}/{impl}]\")\n",
    "                    try:\n",
    "                        runtime_ns, energy_joules = run_benchmark(impl_path, graph_file)\n",
    "                        performance_entry[\"runtime\"][device][impl] = float(runtime_ns)\n",
    "                        performance_entry[\"energy\"][device][impl] = float(energy_joules)\n",
    "                    except subprocess.CalledProcessError as e:\n",
    "                        print(f\"Benchmark failed for {impl_path}: {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error while benchmarking {impl_path}: could not load graph\")\n",
    "\n",
    "        bgo_result[\"performances\"].append(performance_entry)\n",
    "        results.append(bgo_result)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    display(f\"Benchmarking completed\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the BGOs from the workflow on the sampled graph\n",
    "sampled_graph_benchmark = benchmark_workflow(input_dag, sampled_graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows the benchmarked runtimes and energy consumptions for the sampled graph. It is possible that some BGO implementations are not shown, if there is not enough RAM available. This is not an issue for this tutorial, so you can ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(data, target):\n",
    "    plot_annotated_dag(data, \"Benchmarked performance values on the sampled graph\", target)\n",
    "\n",
    "interact(lambda target: plot_result(sampled_graph_benchmark, target), target=ToggleButtons(options=['runtime', 'energy'],description='Prediction target'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic validation\n",
    "To validate the predictions made by the analytical models and sampling approach, we benchmark the same implementations on the full graph. We then compare the predictions against these benchmarks in the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark the BGOs from the workflow on the full graph\n",
    "full_graph_benchmark = benchmark_workflow(input_dag, graph_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analytical model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_request():\n",
    "    form_data = {\n",
    "        'hardware': json.dumps(hardware),\n",
    "        'bgo_dag': json.dumps(input_dag),\n",
    "        'graph_props': json.dumps(graph_props)\n",
    "    }\n",
    "    evaluate_response = requests.post(url + '/evaluate', data=form_data)\n",
    "    return evaluate_response.text\n",
    "\n",
    "def validate_analytical(target):\n",
    "    prediction = json.loads(evaluate_request())\n",
    "    analytical_prediction_validation(prediction, full_graph_benchmark, target)\n",
    "\n",
    "    \n",
    "interact(validate_analytical, target=ToggleButtons(options=['runtime', 'energy'],description='Prediction target'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sampling based prediction validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sampling(data, target):\n",
    "    sampling_prediction_validation(data, full_graph_benchmark, sampling_rate, target)\n",
    "\n",
    "interact(lambda target: validate_sampling(sampled_graph_benchmark, target), target=ToggleButtons(options=['runtime', 'energy'],description='Prediction target'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop prediction server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the server\n",
    "server.terminate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
